# 信息熵(entropy)

## 举个例子   
想象以下场景，我们去游乐场玩一个猜球游戏，店家随机拿出一个球让我们猜颜色，每猜一次(无论对错)奖品的等级就会下降一级，当猜对时游戏结束我们获得当前等级的游戏奖品，若一个盒子中有**蓝**，**紫**，**橙**，**绿**四种颜色的球。那么我们的最少猜球策略和次数是怎么样的？  
场景1：我们对于盒子内球的分布一无所知：  
这个情况，我们对于这个游戏的不确定度是最高的，因此假设所有球的概率分布都是$\frac{1}{4}$
那么我们策略可以是这样的：

<div align=center><img src="../Resource/entropy_strategy_1.jpg "/></div>  

那么在这种情况下我们需要猜$\frac{1}{4}\times2+\frac{1}{4}\times2+\frac{1}{4}\times2+\frac{1}{4}\times2=2$次，可见在这个场景下我们最少需要两次能知道那个球是什么颜色。  
场景2：店家告诉我们盒子内球的概率分布是橙色$\frac{1}{2}$，紫色$\frac{1}{4}$,蓝色$\frac{1}{8}$,绿色$\frac{1}{8}$  
那么这情况下我们的策略应该是这样的：  

<div align=center><img src="../Resource/entropy_strategy_2.jpg "/></div>  

在这种情况下我们需要猜$\frac{1}{2}\times1+\frac{1}{4}\times2+\frac{1}{8}\times3+\frac{1}{8}\times3=1.75$次，我们至少需要1.75次才能猜对颜色，这里可以看到由于我们知道了盒子内的球的分布情况我们所需的次数减少了（虽然实际上还是得两次）。  
场景3：店家大法善心告诉我们盒子里全是橙球：  
这个情况下我们知道橙球的概率是1那么只要猜$1\times0=0$次  

## 信息熵  
经过这个猜球游，我们知道了在最佳策略下我们需要的猜球次数,那么这个**次数**就是**信息熵**，这里可以理解为我们为了要消除**不确定性**所需要的**最小努力**， 上面那些计算式子中的相加项例如场景而下橙球的次数$\frac{1}{2}\times1$称之为单个事件的**自信息**，就是在一个系统内确定某一事件所需要的**最小努力**，把系统内所有的自信息加起来就得到了**信息熵**  
自信息公式：  
$$I(x)=-\log{P(x)}$$  
其中：$P(x)$为事件发生的概率，**注意**：此处的$\log$是没有底数的，而在我们的例子中底数为2,以各种数为底的具体含义见`参考1`  
信息熵公式:  
$$H(x)=-\sum^{N}_{i=1}P(x_i)\log{P(x_i)}$$  
信息熵是一个信息的不确定度的度量，一条信息不确定越高其信息熵越大，在例子中不知道箱子内球的分布即为不确定度最大的情况，因此此情况下的信息熵也最大。而知道全部球都是橙球的情况下不确定度最小，因此此情况下信息熵为0，即对于一个系统而已其信息熵的范围为**完全确定**情况下的信息熵到**完全不确定**情况下的信息熵。  

## 交叉熵  
有时候我们并不总能知道某个系统中事件的正确分布，比如在开头的例子中，老板骗我们球的分布是第二种情况的，但实际上呢是第一种分布，这个时候我们在使用**策略2**情况下所做的最小努力就会变为$\frac{1}{2}\times2+\frac{1}{4}\times2+\frac{1}{8}\times2+\frac{1}{8}\times2=2$可以看到在情景1的分布下使用策略2会让信息熵变为2比原来的1.75大，而这种情况下计算得出的熵就是交叉熵。   
其计算公式为：  
$$H(p,q)=-\sum^{N}_{i=1}p(x_i)\log{q(x_i)}$$  
其中$p$为真正的分布而$q$为非真实分布(即我们使用的策略的分布)  
交叉熵的含义可以理解为我们用一个分布去模拟真正的分布从而得到的信息熵  
P.S.当q=p时交叉熵=信息熵  
## K-L散度  
K-L散度又称相对熵或者K-L距离(只是有这个说法，但并不是真正意义上的距离下文会举例说明)是用来度量我们用来模拟的分布相比于原来的分布损失了多少信息或者说增加了多少不确定度。  
K-L散度公式：  
$$D_{KL}(p||q)=H(p,q)-H(p)$$
即交叉熵-信息熵，展开化简后为：  
$$D_{KL}(p||q)=\sum^{N}_{i=1}p(x_i)\log\frac{p(x_i)}{q(x_i)}$$  
具体举例子来说就是我们交叉熵里的例子减去我们开头情形2的例子即$2-1.75=0.25$  
### K-L 散度的作用  
在现实生活中有许多奇奇怪怪的分布，而我们只总结了有限的分布情况比如：正态分布，多项式分布，泊松分布。那么在遇到这某一种奇怪的分布时我们会用一个已知的分布来去拟合这个新的分布，由上文我们知道这种情况下我们会得到**交叉熵**，此时我们就用K-L散度来描述损失了多少信息，K-L散度的值越小表明我们用来拟合的分布效果越好。  
在机器学习中的**多分类**任务中我们常用的损失函数**交叉熵损失函数**其实就是K-L散度的等价因为训练集的分布即信息熵是不变的，我们最小化交叉熵的过程就是不断的去拟合真实的数据分布。


## 参考  
1.  [wiki信息熵](https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA))
2. [知乎](https://www.zhihu.com/question/41252833)
3.  [简述](https://www.jianshu.com/p/43318a3dc715)